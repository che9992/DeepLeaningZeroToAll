{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2\n",
    "\n",
    "## Predicting exam score: regression\n",
    "\n",
    "학습량과 점수와의 관계\n",
    "\n",
    "| x(hours) \t| y(score) \t|\n",
    "|:--------:\t|----------\t|\n",
    "| 10       \t| 90       \t|\n",
    "| 9        \t| 80       \t|\n",
    "| 3        \t| 50       \t|\n",
    "| 2        \t| 30       \t|\n",
    "\n",
    "1. Linear hypothesis, 그래프를 그려서 선을 그리는 , 이 데이터에 어떤 선이 잘 맞는가 ..\n",
    "\n",
    "2. \\\\(H(x) = Wx + b\\\\) 와 같은 일차 방정식의 선을 그려서 가장 잘 맞는 선을 알아낸다 ( W, b 값 )\n",
    "\n",
    "3. Cost function (Loss function)  How fit the line to our (training) data\n",
    "\n",
    "\n",
    "### GOAL : Minimize cost ! \n",
    "minimize cost (W,b)\n",
    "\n",
    "<script type=\"text/javascript\"  src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "## Hypothesis and Cost \n",
    "\n",
    "\\\\(H(x) = Wx + b \\\\)\n",
    "\n",
    "\\\\( cost(W,b) = \\frac{1}{m}\\sum_{i=1}^{m} (H(x^i) - y^i)^2 \\\\)\n",
    "\n",
    "## Simplified hypothesis\n",
    "\n",
    "\\\\(H(x) = Wx\\\\)\n",
    "\n",
    "\\\\( cost(W) = \\frac{1}{m}\\sum_{i=1}^{m} (Wx^i - y^i)^2 \\\\)\n",
    "\n",
    "\n",
    "##### example\n",
    "\n",
    "x = [1,2,3] , y = [1,2,3]\n",
    "\n",
    "W = 1, cost(W) = 0\n",
    "\n",
    "\\\\(\\frac{1}{3}((1-1)^2+(2-2)^2+(3-3)^2)\\\\)\n",
    "\n",
    "\n",
    "W = 2, cost(w) = 4.67\n",
    "\n",
    "\\\\(\\frac{1}{3}((2-1)^2+(4-2)^2+(6-3)^2)\\\\)\n",
    "\n",
    "위 값들로 cost function 을 그릴수있다.\n",
    "\n",
    "즉, W = 1 일때, minimize cost 를 갖는다.\n",
    "<hr/>\n",
    "\n",
    "### using! Gradient descent algorithm\n",
    "경사를(Gradient) 따라서 내려가는 알고리즘\n",
    "* Start with initial guesses\n",
    "    * start at 0,0 (or any other value)\n",
    "    * keeping changing W and b a little bit to try and reduce cost(W,b)\n",
    "* Each time you change the prameters, you select the gradient which reduces cost(W,b) the most possible\n",
    "* Repeat\n",
    "\n",
    "#### How to find a gradient\n",
    "* Formal definition\n",
    "\n",
    "\\\\( cost(W,b) = \\frac{1}{m}\\sum_{i=1}^{m} (H(x^i) - y^i)^2 \\\\)\n",
    "\n",
    " &darr;&darr;  for differential easily\n",
    "\n",
    "\\\\( cost(W,b) = \\frac{1}{2m}\\sum_{i=1}^{m} (H(x^i) - y^i)^2 \\\\)\n",
    "\n",
    "##### Important ! Convex function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "### Building graph using TF operations\n",
    "\n",
    "\\\\(H(x) = Wx + b\\\\)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x_train = [1,2,3]\n",
    "y_train = [1,2,3]\n",
    "\n",
    "W = tf.Variable(tf.random_normal([1]), name = 'weight') \n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')\n",
    "\n",
    "hypothesis = x_train * W + b\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.Variable \n",
    "같은 변수 선언이지만, 텐션 플로우가 자체적으로 트레이닝에 알맞게 변화시키는? 변수라고 이해하면됨\n",
    "\n",
    "tf.Variable(shape, name)\n",
    "\n",
    "### tf.randome_normal([ 1 ])\n",
    "1차원 랜덤\n",
    "\n",
    "<hr/>\n",
    "### Our hypothesis XW + b\n",
    "hypothesis = x_train * W + b\n",
    "\n",
    "### Cost/Loss Function \n",
    "\\\\( cost(W,b) = \\frac{1}{m}\\sum_{i=1}^{m} (H(x^i) - y^i)^2 \\\\)\n",
    "\n",
    "&darr;&darr;\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example reduce_mean \n",
    "a = [1,2,3,4.]\n",
    "t = tf.reduce_mean(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(cost) # tf.Variable 을 알아서 조정해서 minimize 함 , node 구성 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     train \n",
    "       |\n",
    "     cost\n",
    "       |\n",
    "    hypothesis\n",
    "     |      |\n",
    "     W      b\n",
    "     \n",
    "위 처럼 train 이 구성되어 있어서, 아래에 train을 세션에 넣어 실행시 위와같은 node를 타고 내려가 W 와 b를 조작하며 실행된다고 이해하면됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기까지가 그래프 구현 \n",
    "### End building graph \n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run / update graph and get results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.767453 [0.38025331] [-0.34524482]\n",
      "20 0.025569186 [0.9639468] [-0.08506431]\n",
      "40 0.0006654994 [1.0184468] [-0.0578374]\n",
      "60 0.00039990642 [1.022608] [-0.05290745]\n",
      "80 0.0003613479 [1.0220242] [-0.05021045]\n",
      "100 0.0003281663 [1.0210347] [-0.04783067]\n",
      "120 0.00029804235 [1.0200504] [-0.04558087]\n",
      "140 0.00027068768 [1.0191085] [-0.04343849]\n",
      "160 0.000245842 [1.0182105] [-0.04139698]\n",
      "180 0.00022327846 [1.0173546] [-0.03945144]\n",
      "200 0.00020278251 [1.0165391] [-0.03759731]\n",
      "220 0.00018417234 [1.0157619] [-0.03583037]\n",
      "240 0.00016726606 [1.0150211] [-0.03414646]\n",
      "260 0.0001519163 [1.0143152] [-0.03254172]\n",
      "280 0.00013797352 [1.0136424] [-0.03101239]\n",
      "300 0.00012530827 [1.0130012] [-0.02955491]\n",
      "320 0.00011380753 [1.0123903] [-0.0281659]\n",
      "340 0.00010336103 [1.0118079] [-0.0268422]\n",
      "360 9.387466e-05 [1.011253] [-0.0255807]\n",
      "380 8.5258296e-05 [1.0107243] [-0.02437855]\n",
      "400 7.743325e-05 [1.0102202] [-0.02323289]\n",
      "420 7.0324255e-05 [1.0097398] [-0.02214094]\n",
      "440 6.386986e-05 [1.009282] [-0.02110036]\n",
      "460 5.8007834e-05 [1.0088458] [-0.0201087]\n",
      "480 5.2683947e-05 [1.0084301] [-0.01916363]\n",
      "500 4.7848735e-05 [1.0080339] [-0.01826298]\n",
      "520 4.3456574e-05 [1.0076563] [-0.01740465]\n",
      "540 3.946735e-05 [1.0072966] [-0.01658671]\n",
      "560 3.58453e-05 [1.0069536] [-0.01580724]\n",
      "580 3.2555476e-05 [1.0066267] [-0.01506431]\n",
      "600 2.9566414e-05 [1.0063152] [-0.01435629]\n",
      "620 2.6852043e-05 [1.0060185] [-0.01368155]\n",
      "640 2.4388142e-05 [1.0057358] [-0.01303862]\n",
      "660 2.2150132e-05 [1.0054662] [-0.01242588]\n",
      "680 2.0116777e-05 [1.0052093] [-0.01184196]\n",
      "700 1.8271267e-05 [1.0049645] [-0.01128542]\n",
      "720 1.6593376e-05 [1.004731] [-0.01075503]\n",
      "740 1.5070509e-05 [1.0045089] [-0.01024959]\n",
      "760 1.3687261e-05 [1.004297] [-0.00976791]\n",
      "780 1.2431677e-05 [1.0040951] [-0.00930893]\n",
      "800 1.12904245e-05 [1.0039027] [-0.00887149]\n",
      "820 1.0254011e-05 [1.0037192] [-0.00845458]\n",
      "840 9.3127865e-06 [1.0035443] [-0.00805725]\n",
      "860 8.458624e-06 [1.0033779] [-0.00767861]\n",
      "880 7.682089e-06 [1.0032191] [-0.00731778]\n",
      "900 6.976632e-06 [1.0030679] [-0.00697387]\n",
      "920 6.3367115e-06 [1.0029237] [-0.00664615]\n",
      "940 5.7552593e-06 [1.0027863] [-0.00633381]\n",
      "960 5.2269856e-06 [1.0026553] [-0.00603614]\n",
      "980 4.747076e-06 [1.0025306] [-0.00575248]\n",
      "1000 4.311367e-06 [1.0024116] [-0.00548214]\n",
      "1020 3.9155034e-06 [1.0022982] [-0.00522449]\n",
      "1040 3.5564187e-06 [1.0021904] [-0.00497898]\n",
      "1060 3.2298778e-06 [1.0020875] [-0.00474503]\n",
      "1080 2.9336168e-06 [1.0019894] [-0.0045221]\n",
      "1100 2.6643702e-06 [1.0018959] [-0.00430961]\n",
      "1120 2.4197782e-06 [1.0018067] [-0.00410712]\n",
      "1140 2.1980816e-06 [1.001722] [-0.00391418]\n",
      "1160 1.996238e-06 [1.001641] [-0.00373029]\n",
      "1180 1.8129849e-06 [1.0015639] [-0.00355505]\n",
      "1200 1.6468672e-06 [1.0014906] [-0.00338804]\n",
      "1220 1.4956813e-06 [1.0014205] [-0.00322893]\n",
      "1240 1.3584167e-06 [1.0013537] [-0.00307722]\n",
      "1260 1.2337998e-06 [1.0012903] [-0.00293269]\n",
      "1280 1.1206349e-06 [1.0012296] [-0.00279497]\n",
      "1300 1.0179104e-06 [1.0011718] [-0.0026637]\n",
      "1320 9.2450006e-07 [1.0011168] [-0.00253863]\n",
      "1340 8.396859e-07 [1.0010643] [-0.0024194]\n",
      "1360 7.628242e-07 [1.0010144] [-0.00230582]\n",
      "1380 6.9292406e-07 [1.0009667] [-0.0021976]\n",
      "1400 6.2934146e-07 [1.0009214] [-0.00209445]\n",
      "1420 5.716636e-07 [1.0008783] [-0.0019962]\n",
      "1440 5.1924616e-07 [1.0008372] [-0.00190253]\n",
      "1460 4.7176096e-07 [1.0007979] [-0.00181322]\n",
      "1480 4.2845105e-07 [1.0007603] [-0.0017281]\n",
      "1500 3.891818e-07 [1.0007246] [-0.00164705]\n",
      "1520 3.5349913e-07 [1.0006908] [-0.00156976]\n",
      "1540 3.2111356e-07 [1.0006583] [-0.00149606]\n",
      "1560 2.9166e-07 [1.0006273] [-0.00142592]\n",
      "1580 2.6499518e-07 [1.0005982] [-0.00135901]\n",
      "1600 2.4067324e-07 [1.0005698] [-0.00129529]\n",
      "1620 2.1869808e-07 [1.0005435] [-0.00123461]\n",
      "1640 1.986637e-07 [1.0005176] [-0.0011767]\n",
      "1660 1.8052509e-07 [1.0004938] [-0.00112161]\n",
      "1680 1.6394996e-07 [1.0004703] [-0.001069]\n",
      "1700 1.4893676e-07 [1.0004486] [-0.00101892]\n",
      "1720 1.3530088e-07 [1.0004271] [-0.00097119]\n",
      "1740 1.2293974e-07 [1.0004075] [-0.0009256]\n",
      "1760 1.11716815e-07 [1.0003884] [-0.00088236]\n",
      "1780 1.0145025e-07 [1.00037] [-0.00084091]\n",
      "1800 9.222358e-08 [1.000353] [-0.00080155]\n",
      "1820 8.3795065e-08 [1.0003363] [-0.00076413]\n",
      "1840 7.608293e-08 [1.0003204] [-0.00072819]\n",
      "1860 6.914882e-08 [1.0003057] [-0.00069411]\n",
      "1880 6.281869e-08 [1.0002913] [-0.00066175]\n",
      "1900 5.7054915e-08 [1.0002773] [-0.00063065]\n",
      "1920 5.181818e-08 [1.0002645] [-0.00060104]\n",
      "1940 4.7147193e-08 [1.0002525] [-0.00057303]\n",
      "1960 4.2831036e-08 [1.0002406] [-0.00054637]\n",
      "1980 3.887476e-08 [1.0002289] [-0.00052067]\n",
      "2000 3.532089e-08 [1.0002184] [-0.00049618]\n"
     ]
    }
   ],
   "source": [
    "# 그래프를 넣을 세션 \n",
    "sess = tf.Session()\n",
    "# tf.Variable로 변수 선언 후 에는 반드시 아래 코드로 init 이 필요함\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#\n",
    "for step in range(2001):\n",
    "    sess.run(train) # 위에 만든 node 실행 \n",
    "    if step % 20 == 0 :\n",
    "        print(step, sess.run(cost), sess.run(W), sess.run(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
